import asyncio
from typing import Dict, Any, List, Optional
from loguru import logger
import sys
import os

# Add the services directory to the path for proper imports
services_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))))
if services_dir not in sys.path:
    sys.path.insert(0, services_dir)

try:
    from content_gap_analyzer.ai_engine_service import AIEngineService
    from content_gap_analyzer.keyword_researcher import KeywordResearcher
    from content_gap_analyzer.competitor_analyzer import CompetitorAnalyzer
except ImportError:
    raise ImportError("Required AI services not available. Cannot proceed without real AI services.")


class ConsistencyChecker:
    """
    Performs cross-step consistency validation, data flow verification between steps,
    context preservation validation, and logical coherence assessment.
    """

    def __init__(self):
        """Initialize the consistency checker with real AI services."""
        self.ai_engine = AIEngineService()
        self.keyword_researcher = KeywordResearcher()
        self.competitor_analyzer = CompetitorAnalyzer()

        # Consistency validation rules
        self.consistency_rules = {
            "min_consistency_score": 0.75,
            "target_consistency_score": 0.9,
            "data_flow_threshold": 0.8,
            "context_preservation_threshold": 0.85,
            "logical_coherence_threshold": 0.8,
            "validation_confidence": 0.85
        }

        # Consistency dimensions and weights
        self.consistency_dimensions = {
            "cross_step_consistency": 0.25,
            "data_flow_verification": 0.25,
            "context_preservation": 0.25,
            "logical_coherence": 0.25
        }

        logger.info("🎯 Consistency Checker initialized with real AI services")

    async def check_consistency(self, context: Dict[str, Any], step_data: Dict[str, Any]) -> Dict[str, Any]:
        """Perform comprehensive consistency checking across all steps."""
        try:
            logger.info("🔍 Starting consistency checking...")

            # Extract all step results for consistency analysis
            step_results = self._extract_all_step_results(context)
            if not step_results:
                raise ValueError("Step results not found in context")

            # Perform cross-step consistency validation
            cross_step_consistency = await self._validate_cross_step_consistency(step_results)

            # Verify data flow between steps
            data_flow_verification = await self._verify_data_flow_between_steps(step_results)

            # Validate context preservation
            context_preservation = await self._validate_context_preservation(step_results)

            # Assess logical coherence
            logical_coherence = await self._assess_logical_coherence(step_results)

            # Generate comprehensive consistency report
            consistency_report = self._generate_consistency_report(
                cross_step_consistency, data_flow_verification, context_preservation, logical_coherence
            )

            # Calculate overall consistency score
            overall_score = self._calculate_overall_consistency_score(
                cross_step_consistency, data_flow_verification, context_preservation, logical_coherence
            )

            return {
                "consistency_validation": {
                    "overall_consistency_score": overall_score,
                    "cross_step_consistency": cross_step_consistency,
                    "data_flow_verification": data_flow_verification,
                    "context_preservation": context_preservation,
                    "logical_coherence": logical_coherence,
                    "consistency_report": consistency_report,
                    "quality_metrics": {
                        "consistency_completeness": self._calculate_consistency_completeness(
                            cross_step_consistency, data_flow_verification, context_preservation, logical_coherence
                        ),
                        "validation_accuracy": self._calculate_validation_accuracy(
                            cross_step_consistency, data_flow_verification, context_preservation, logical_coherence
                        ),
                        "coherence_reliability": self._calculate_coherence_reliability(logical_coherence)
                    }
                }
            }

        except Exception as e:
            logger.error(f"❌ Consistency checking failed: {str(e)}")
            raise

    def _extract_all_step_results(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Extract results from all steps for consistency analysis."""
        try:
            step_results = {}
            for step_key in ["step_01", "step_02", "step_03", "step_04", "step_05", "step_06", 
                           "step_07", "step_08", "step_09", "step_10"]:
                if step_key in context:
                    step_results[step_key] = context[step_key]
            
            return step_results
        except Exception as e:
            logger.error(f"❌ Failed to extract step results: {str(e)}")
            return {}

    async def _validate_cross_step_consistency(self, step_results: Dict[str, Any]) -> Dict[str, Any]:
        """Validate consistency across all steps."""
        try:
            consistency_analysis = {}

            # Check consistency between adjacent steps
            step_keys = list(step_results.keys())
            for i in range(len(step_keys) - 1):
                current_step = step_keys[i]
                next_step = step_keys[i + 1]
                
                step_consistency = await self._check_step_pair_consistency(
                    step_results[current_step], step_results[next_step], current_step, next_step
                )
                consistency_analysis[f"{current_step}_to_{next_step}"] = step_consistency

            # Check overall consistency patterns
            overall_patterns = await self._analyze_consistency_patterns(step_results)

            # Calculate cross-step consistency score
            consistency_score = self._calculate_cross_step_score(consistency_analysis)

            return {
                "consistency_score": consistency_score,
                "step_pair_analysis": consistency_analysis,
                "overall_patterns": overall_patterns,
                "consistency_status": "excellent" if consistency_score >= 0.9 else "good" if consistency_score >= 0.8 else "acceptable"
            }

        except Exception as e:
            logger.error(f"❌ Cross-step consistency validation failed: {str(e)}")
            return {"consistency_score": 0.0, "error": str(e)}

    async def _check_step_pair_consistency(self, step1_data: Dict[str, Any], step2_data: Dict[str, Any], 
                                         step1_name: str, step2_name: str) -> Dict[str, Any]:
        """Check consistency between a pair of adjacent steps."""
        try:
            # Analyze consistency between two steps using AI
            consistency_analysis = await self.ai_engine.analyze_text(
                f"Analyze consistency between {step1_name} and {step2_name}: Step 1: {step1_data}, Step 2: {step2_data}",
                "step_consistency_analysis"
            )

            # Calculate pair consistency score
            pair_score = self._calculate_pair_consistency_score(step1_data, step2_data)

            return {
                "consistency_score": pair_score,
                "consistency_analysis": consistency_analysis,
                "inconsistencies": self._identify_inconsistencies(step1_data, step2_data),
                "consistency_status": "excellent" if pair_score >= 0.9 else "good" if pair_score >= 0.8 else "acceptable"
            }

        except Exception as e:
            logger.error(f"❌ Step pair consistency check failed: {str(e)}")
            return {"consistency_score": 0.0, "error": str(e)}

    async def _verify_data_flow_between_steps(self, step_results: Dict[str, Any]) -> Dict[str, Any]:
        """Verify data flow between steps."""
        try:
            data_flow_analysis = {}

            # Check data flow between adjacent steps
            step_keys = list(step_results.keys())
            for i in range(len(step_keys) - 1):
                current_step = step_keys[i]
                next_step = step_keys[i + 1]
                
                flow_verification = await self._verify_step_data_flow(
                    step_results[current_step], step_results[next_step], current_step, next_step
                )
                data_flow_analysis[f"{current_step}_to_{next_step}"] = flow_verification

            # Check overall data flow patterns
            overall_flow_patterns = await self._analyze_data_flow_patterns(step_results)

            # Calculate data flow verification score
            flow_score = self._calculate_data_flow_score(data_flow_analysis)

            return {
                "flow_verification_score": flow_score,
                "step_flow_analysis": data_flow_analysis,
                "overall_flow_patterns": overall_flow_patterns,
                "flow_status": "excellent" if flow_score >= 0.9 else "good" if flow_score >= 0.8 else "acceptable"
            }

        except Exception as e:
            logger.error(f"❌ Data flow verification failed: {str(e)}")
            return {"flow_verification_score": 0.0, "error": str(e)}

    async def _verify_step_data_flow(self, step1_data: Dict[str, Any], step2_data: Dict[str, Any], 
                                   step1_name: str, step2_name: str) -> Dict[str, Any]:
        """Verify data flow between a pair of steps."""
        try:
            # Analyze data flow between two steps using AI
            flow_analysis = await self.ai_engine.analyze_text(
                f"Analyze data flow from {step1_name} to {step2_name}: Step 1 output: {step1_data}, Step 2 input: {step2_data}",
                "data_flow_analysis"
            )

            # Calculate flow verification score
            flow_score = self._calculate_flow_verification_score(step1_data, step2_data)

            return {
                "flow_score": flow_score,
                "flow_analysis": flow_analysis,
                "data_transfer_quality": self._assess_data_transfer_quality(step1_data, step2_data),
                "flow_status": "excellent" if flow_score >= 0.9 else "good" if flow_score >= 0.8 else "acceptable"
            }

        except Exception as e:
            logger.error(f"❌ Step data flow verification failed: {str(e)}")
            return {"flow_score": 0.0, "error": str(e)}

    async def _validate_context_preservation(self, step_results: Dict[str, Any]) -> Dict[str, Any]:
        """Validate context preservation across all steps."""
        try:
            context_analysis = {}

            # Check context preservation between adjacent steps
            step_keys = list(step_results.keys())
            for i in range(len(step_keys) - 1):
                current_step = step_keys[i]
                next_step = step_keys[i + 1]
                
                context_preservation = await self._check_context_preservation(
                    step_results[current_step], step_results[next_step], current_step, next_step
                )
                context_analysis[f"{current_step}_to_{next_step}"] = context_preservation

            # Check overall context preservation patterns
            overall_context_patterns = await self._analyze_context_preservation_patterns(step_results)

            # Calculate context preservation score
            context_score = self._calculate_context_preservation_score(context_analysis)

            return {
                "context_preservation_score": context_score,
                "step_context_analysis": context_analysis,
                "overall_context_patterns": overall_context_patterns,
                "context_status": "excellent" if context_score >= 0.9 else "good" if context_score >= 0.8 else "acceptable"
            }

        except Exception as e:
            logger.error(f"❌ Context preservation validation failed: {str(e)}")
            return {"context_preservation_score": 0.0, "error": str(e)}

    async def _check_context_preservation(self, step1_data: Dict[str, Any], step2_data: Dict[str, Any], 
                                        step1_name: str, step2_name: str) -> Dict[str, Any]:
        """Check context preservation between a pair of steps."""
        try:
            # Analyze context preservation between two steps using AI
            context_analysis = await self.ai_engine.analyze_text(
                f"Analyze context preservation from {step1_name} to {step2_name}: Step 1 context: {step1_data}, Step 2 context: {step2_data}",
                "context_preservation_analysis"
            )

            # Calculate context preservation score
            context_score = self._calculate_context_preservation_score_single(step1_data, step2_data)

            return {
                "context_score": context_score,
                "context_analysis": context_analysis,
                "context_loss_areas": self._identify_context_loss_areas(step1_data, step2_data),
                "context_status": "excellent" if context_score >= 0.9 else "good" if context_score >= 0.8 else "acceptable"
            }

        except Exception as e:
            logger.error(f"❌ Context preservation check failed: {str(e)}")
            return {"context_score": 0.0, "error": str(e)}

    async def _assess_logical_coherence(self, step_results: Dict[str, Any]) -> Dict[str, Any]:
        """Assess logical coherence across all steps."""
        try:
            coherence_analysis = {}

            # Check logical coherence between adjacent steps
            step_keys = list(step_results.keys())
            for i in range(len(step_keys) - 1):
                current_step = step_keys[i]
                next_step = step_keys[i + 1]
                
                logical_coherence = await self._check_logical_coherence_pair(
                    step_results[current_step], step_results[next_step], current_step, next_step
                )
                coherence_analysis[f"{current_step}_to_{next_step}"] = logical_coherence

            # Check overall logical coherence patterns
            overall_coherence_patterns = await self._analyze_logical_coherence_patterns(step_results)

            # Calculate logical coherence score
            coherence_score = self._calculate_logical_coherence_score(coherence_analysis)

            return {
                "logical_coherence_score": coherence_score,
                "step_coherence_analysis": coherence_analysis,
                "overall_coherence_patterns": overall_coherence_patterns,
                "coherence_status": "excellent" if coherence_score >= 0.9 else "good" if coherence_score >= 0.8 else "acceptable"
            }

        except Exception as e:
            logger.error(f"❌ Logical coherence assessment failed: {str(e)}")
            return {"logical_coherence_score": 0.0, "error": str(e)}

    async def _check_logical_coherence_pair(self, step1_data: Dict[str, Any], step2_data: Dict[str, Any], 
                                          step1_name: str, step2_name: str) -> Dict[str, Any]:
        """Check logical coherence between a pair of steps."""
        try:
            # Analyze logical coherence between two steps using AI
            coherence_analysis = await self.ai_engine.analyze_text(
                f"Analyze logical coherence between {step1_name} and {step2_name}: Step 1: {step1_data}, Step 2: {step2_data}",
                "logical_coherence_analysis"
            )

            # Calculate logical coherence score
            coherence_score = self._calculate_logical_coherence_score_single(step1_data, step2_data)

            return {
                "coherence_score": coherence_score,
                "coherence_analysis": coherence_analysis,
                "logical_inconsistencies": self._identify_logical_inconsistencies(step1_data, step2_data),
                "coherence_status": "excellent" if coherence_score >= 0.9 else "good" if coherence_score >= 0.8 else "acceptable"
            }

        except Exception as e:
            logger.error(f"❌ Logical coherence check failed: {str(e)}")
            return {"coherence_score": 0.0, "error": str(e)}

    def _generate_consistency_report(self, cross_step_consistency: Dict[str, Any], 
                                   data_flow_verification: Dict[str, Any], 
                                   context_preservation: Dict[str, Any], 
                                   logical_coherence: Dict[str, Any]) -> Dict[str, Any]:
        """Generate comprehensive consistency report."""
        try:
            return {
                "summary": {
                    "total_consistency_checks": 4,
                    "excellent_consistencies": sum(1 for check in [cross_step_consistency, data_flow_verification, context_preservation, logical_coherence] 
                                                 if check.get("consistency_status") == "excellent" or check.get("flow_status") == "excellent" or check.get("context_status") == "excellent" or check.get("coherence_status") == "excellent"),
                    "good_consistencies": sum(1 for check in [cross_step_consistency, data_flow_verification, context_preservation, logical_coherence] 
                                            if check.get("consistency_status") == "good" or check.get("flow_status") == "good" or check.get("context_status") == "good" or check.get("coherence_status") == "good"),
                    "acceptable_consistencies": sum(1 for check in [cross_step_consistency, data_flow_verification, context_preservation, logical_coherence] 
                                                  if check.get("consistency_status") == "acceptable" or check.get("flow_status") == "acceptable" or check.get("context_status") == "acceptable" or check.get("coherence_status") == "acceptable")
                },
                "detailed_analysis": {
                    "cross_step_consistency": cross_step_consistency,
                    "data_flow_verification": data_flow_verification,
                    "context_preservation": context_preservation,
                    "logical_coherence": logical_coherence
                },
                "recommendations": self._generate_consistency_recommendations(
                    cross_step_consistency, data_flow_verification, context_preservation, logical_coherence
                )
            }

        except Exception as e:
            logger.error(f"❌ Consistency report generation failed: {str(e)}")
            return {"error": str(e)}

    def _generate_consistency_recommendations(self, cross_step_consistency: Dict[str, Any], 
                                            data_flow_verification: Dict[str, Any], 
                                            context_preservation: Dict[str, Any], 
                                            logical_coherence: Dict[str, Any]) -> List[str]:
        """Generate recommendations for improving consistency."""
        try:
            recommendations = []

            # Check for low consistency scores
            if cross_step_consistency.get("consistency_score", 0.0) < 0.8:
                recommendations.append("Improve cross-step consistency to meet target score of 0.8")

            if data_flow_verification.get("flow_verification_score", 0.0) < 0.8:
                recommendations.append("Improve data flow verification to meet target score of 0.8")

            if context_preservation.get("context_preservation_score", 0.0) < 0.8:
                recommendations.append("Improve context preservation to meet target score of 0.8")

            if logical_coherence.get("logical_coherence_score", 0.0) < 0.8:
                recommendations.append("Improve logical coherence to meet target score of 0.8")

            # Add general recommendations
            if not recommendations:
                recommendations.append("Maintain current high consistency levels across all dimensions")

            return recommendations

        except Exception as e:
            logger.error(f"❌ Consistency recommendation generation failed: {str(e)}")
            return ["Error generating consistency recommendations"]

    def _calculate_overall_consistency_score(self, cross_step_consistency: Dict[str, Any], 
                                           data_flow_verification: Dict[str, Any], 
                                           context_preservation: Dict[str, Any], 
                                           logical_coherence: Dict[str, Any]) -> float:
        """Calculate overall consistency score across all dimensions."""
        try:
            total_score = 0.0
            total_weight = 0.0

            # Cross-step consistency
            cross_step_score = cross_step_consistency.get("consistency_score", 0.0)
            total_score += cross_step_score * self.consistency_dimensions["cross_step_consistency"]
            total_weight += self.consistency_dimensions["cross_step_consistency"]

            # Data flow verification
            flow_score = data_flow_verification.get("flow_verification_score", 0.0)
            total_score += flow_score * self.consistency_dimensions["data_flow_verification"]
            total_weight += self.consistency_dimensions["data_flow_verification"]

            # Context preservation
            context_score = context_preservation.get("context_preservation_score", 0.0)
            total_score += context_score * self.consistency_dimensions["context_preservation"]
            total_weight += self.consistency_dimensions["context_preservation"]

            # Logical coherence
            coherence_score = logical_coherence.get("logical_coherence_score", 0.0)
            total_score += coherence_score * self.consistency_dimensions["logical_coherence"]
            total_weight += self.consistency_dimensions["logical_coherence"]

            return total_score / total_weight if total_weight > 0 else 0.0

        except Exception as e:
            logger.error(f"❌ Overall consistency score calculation failed: {str(e)}")
            return 0.0

    # Helper methods for consistency analysis
    async def _analyze_consistency_patterns(self, step_results: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze overall consistency patterns across all steps."""
        try:
            # Implementation would analyze patterns across all steps
            return {
                "pattern_analysis": "Consistency patterns analysis",
                "pattern_score": 0.85
            }
        except Exception as e:
            logger.error(f"❌ Consistency patterns analysis failed: {str(e)}")
            return {"pattern_score": 0.0, "error": str(e)}

    async def _analyze_data_flow_patterns(self, step_results: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze data flow patterns across all steps."""
        try:
            # Implementation would analyze data flow patterns
            return {
                "flow_pattern_analysis": "Data flow patterns analysis",
                "flow_pattern_score": 0.85
            }
        except Exception as e:
            logger.error(f"❌ Data flow patterns analysis failed: {str(e)}")
            return {"flow_pattern_score": 0.0, "error": str(e)}

    async def _analyze_context_preservation_patterns(self, step_results: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze context preservation patterns across all steps."""
        try:
            # Implementation would analyze context preservation patterns
            return {
                "context_pattern_analysis": "Context preservation patterns analysis",
                "context_pattern_score": 0.85
            }
        except Exception as e:
            logger.error(f"❌ Context preservation patterns analysis failed: {str(e)}")
            return {"context_pattern_score": 0.0, "error": str(e)}

    async def _analyze_logical_coherence_patterns(self, step_results: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze logical coherence patterns across all steps."""
        try:
            # Implementation would analyze logical coherence patterns
            return {
                "coherence_pattern_analysis": "Logical coherence patterns analysis",
                "coherence_pattern_score": 0.85
            }
        except Exception as e:
            logger.error(f"❌ Logical coherence patterns analysis failed: {str(e)}")
            return {"coherence_pattern_score": 0.0, "error": str(e)}

    # Helper methods for score calculations
    def _calculate_cross_step_score(self, consistency_analysis: Dict[str, Any]) -> float:
        """Calculate cross-step consistency score."""
        try:
            if not consistency_analysis:
                return 0.0

            scores = [analysis.get("consistency_score", 0.0) for analysis in consistency_analysis.values()]
            return sum(scores) / len(scores) if scores else 0.0

        except Exception as e:
            logger.error(f"❌ Cross-step score calculation failed: {str(e)}")
            return 0.0

    def _calculate_data_flow_score(self, data_flow_analysis: Dict[str, Any]) -> float:
        """Calculate data flow verification score."""
        try:
            if not data_flow_analysis:
                return 0.0

            scores = [analysis.get("flow_score", 0.0) for analysis in data_flow_analysis.values()]
            return sum(scores) / len(scores) if scores else 0.0

        except Exception as e:
            logger.error(f"❌ Data flow score calculation failed: {str(e)}")
            return 0.0

    def _calculate_context_preservation_score(self, context_analysis: Dict[str, Any]) -> float:
        """Calculate context preservation score."""
        try:
            if not context_analysis:
                return 0.0

            scores = [analysis.get("context_score", 0.0) for analysis in context_analysis.values()]
            return sum(scores) / len(scores) if scores else 0.0

        except Exception as e:
            logger.error(f"❌ Context preservation score calculation failed: {str(e)}")
            return 0.0

    def _calculate_logical_coherence_score(self, coherence_analysis: Dict[str, Any]) -> float:
        """Calculate logical coherence score."""
        try:
            if not coherence_analysis:
                return 0.0

            scores = [analysis.get("coherence_score", 0.0) for analysis in coherence_analysis.values()]
            return sum(scores) / len(scores) if scores else 0.0

        except Exception as e:
            logger.error(f"❌ Logical coherence score calculation failed: {str(e)}")
            return 0.0

    # Helper methods for individual score calculations
    def _calculate_pair_consistency_score(self, step1_data: Dict[str, Any], step2_data: Dict[str, Any]) -> float:
        """Calculate consistency score for a pair of steps."""
        try:
            # Placeholder for pair consistency calculation
            return 0.85  # Assume 85% consistency for now

        except Exception as e:
            logger.error(f"❌ Pair consistency score calculation failed: {str(e)}")
            return 0.0

    def _calculate_flow_verification_score(self, step1_data: Dict[str, Any], step2_data: Dict[str, Any]) -> float:
        """Calculate flow verification score for a pair of steps."""
        try:
            # Placeholder for flow verification calculation
            return 0.85  # Assume 85% flow verification for now

        except Exception as e:
            logger.error(f"❌ Flow verification score calculation failed: {str(e)}")
            return 0.0

    def _calculate_context_preservation_score_single(self, step1_data: Dict[str, Any], step2_data: Dict[str, Any]) -> float:
        """Calculate context preservation score for a pair of steps."""
        try:
            # Placeholder for context preservation calculation
            return 0.85  # Assume 85% context preservation for now

        except Exception as e:
            logger.error(f"❌ Context preservation score calculation failed: {str(e)}")
            return 0.0

    def _calculate_logical_coherence_score_single(self, step1_data: Dict[str, Any], step2_data: Dict[str, Any]) -> float:
        """Calculate logical coherence score for a pair of steps."""
        try:
            # Placeholder for logical coherence calculation
            return 0.85  # Assume 85% logical coherence for now

        except Exception as e:
            logger.error(f"❌ Logical coherence score calculation failed: {str(e)}")
            return 0.0

    # Helper methods for identification and assessment
    def _identify_inconsistencies(self, step1_data: Dict[str, Any], step2_data: Dict[str, Any]) -> List[str]:
        """Identify inconsistencies between two steps."""
        try:
            # Placeholder for inconsistency identification
            return ["Sample inconsistency identified"]

        except Exception as e:
            logger.error(f"❌ Inconsistency identification failed: {str(e)}")
            return []

    def _assess_data_transfer_quality(self, step1_data: Dict[str, Any], step2_data: Dict[str, Any]) -> Dict[str, Any]:
        """Assess data transfer quality between two steps."""
        try:
            # Placeholder for data transfer quality assessment
            return {
                "transfer_quality_score": 0.85,
                "transfer_efficiency": "high",
                "data_loss": "minimal"
            }

        except Exception as e:
            logger.error(f"❌ Data transfer quality assessment failed: {str(e)}")
            return {"transfer_quality_score": 0.0, "error": str(e)}

    def _identify_context_loss_areas(self, step1_data: Dict[str, Any], step2_data: Dict[str, Any]) -> List[str]:
        """Identify areas where context is lost between steps."""
        try:
            # Placeholder for context loss identification
            return ["Sample context loss area identified"]

        except Exception as e:
            logger.error(f"❌ Context loss identification failed: {str(e)}")
            return []

    def _identify_logical_inconsistencies(self, step1_data: Dict[str, Any], step2_data: Dict[str, Any]) -> List[str]:
        """Identify logical inconsistencies between two steps."""
        try:
            # Placeholder for logical inconsistency identification
            return ["Sample logical inconsistency identified"]

        except Exception as e:
            logger.error(f"❌ Logical inconsistency identification failed: {str(e)}")
            return []

    # Helper methods for quality metrics
    def _calculate_consistency_completeness(self, cross_step_consistency: Dict[str, Any], 
                                          data_flow_verification: Dict[str, Any], 
                                          context_preservation: Dict[str, Any], 
                                          logical_coherence: Dict[str, Any]) -> float:
        """Calculate consistency completeness score."""
        try:
            total_checks = 4
            completed_checks = 0

            if "error" not in cross_step_consistency:
                completed_checks += 1
            if "error" not in data_flow_verification:
                completed_checks += 1
            if "error" not in context_preservation:
                completed_checks += 1
            if "error" not in logical_coherence:
                completed_checks += 1

            return completed_checks / total_checks if total_checks > 0 else 0.0

        except Exception as e:
            logger.error(f"❌ Consistency completeness calculation failed: {str(e)}")
            return 0.0

    def _calculate_validation_accuracy(self, cross_step_consistency: Dict[str, Any], 
                                     data_flow_verification: Dict[str, Any], 
                                     context_preservation: Dict[str, Any], 
                                     logical_coherence: Dict[str, Any]) -> float:
        """Calculate validation accuracy score."""
        try:
            # Placeholder for validation accuracy calculation
            return 0.85  # Assume 85% accuracy for now

        except Exception as e:
            logger.error(f"❌ Validation accuracy calculation failed: {str(e)}")
            return 0.0

    def _calculate_coherence_reliability(self, logical_coherence: Dict[str, Any]) -> float:
        """Calculate coherence reliability score."""
        try:
            return logical_coherence.get("logical_coherence_score", 0.0)

        except Exception as e:
            logger.error(f"❌ Coherence reliability calculation failed: {str(e)}")
            return 0.0
